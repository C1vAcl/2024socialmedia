## 庄钰星
## 23计广 
## 学号：202318093011
# 数据清洗

## 定义
数据清洗是数据预处理的重要步骤，旨在识别并修正数据集中的错误、重复、不完整或不一致的信息，从而提高数据的质量和准确性，使其适合进一步分析或建模。

## 背景
在当今时代,企业信息化的要求越来越迫切,其中一个很重要的方面就是企业数据的管理,根据“进去的是垃圾,出来的也是垃圾(garbage in,garbage out)”这条原理,为了支持正确决策,就要求所管理的数据可靠,没有错误,准确地反映企业的实际情况。[1]数据清洗过程的主要加工处理对象是脏数据。脏数据本身具有的不一致和不准确性等特点,直接影响了数据的显式和隐式价值,即直接影响了数据的质量。良好的数据清洗过程，能有效地剔去脏数据中的糟粕,使其内含的价值显露。因此数据清洗在提高数据质量上起着决定性的作用。[2]

## 举例
主要的数据清洗方法包括：
### 1、聚合 (Aggregation)
将多个特征合并为一个，或改变特征的数量级。例如，可以将图片的宽度和高度合并为面积，以节省存储空间并减少数据维度。[3]
Eg. 在一个图像数据集中，假设我们有两个特征：宽度（width）和高度（height）。我们可以通过计算面积（area）来聚合这两个特征。

df['area'] = df['width'] * df['height']

这样可以减少特征数量，同时也可能提高模型的表现。
### 2、离散化 (Discretization)
将连续特征转化为离散值，如将花费的金钱分为“多”、“正常”和“少”三个等级，这样可以简化模型的处理。[3]
Eg. 假设我们有一个花费特征（spending），其值范围在0到2000之间。我们可以将其分为三个类别。

bins = [0, 500, 1000, float('inf')]
labels = ['少', '正常', '多']
df['spending_category'] = pd.cut(df['spending'], bins=bins, labels=labels)

这样可以简化模型的处理，使得模型更容易学习。
### 3、特征筛选 (Feature Selection)
从所有特征中选取有用的子集，以避免不相关特征导致的计算性能下降。例如，顾客的名字可能对预测购买行为无效。[3]
Eg. 在一个客户数据集中，可能包含客户的姓名、年龄、购买金额等多个特征。我们可以筛选出与目标变量（如购买行为）相关的特征。

df_filtered = df[['age', 'purchase_amount']]  # 只保留有用特征

这样避免了不相关特征对模型性能的影响。
### 4、特征提取 (Feature Extraction)
将现有特征转换为新的特征集，从而提升模型性能。聚合也可视为特征提取的一种形式。[3]
Eg. 假设我们有多个传感器的数据，包括温度（temperature）、湿度（humidity）等。我们可以创建一个新特征，比如热指数（heat index）。

df['heat_index'] = df['temperature'] + df['humidity']  # 伪代码

### 5、取样 (Sampling)
由于处理整个数据集的成本高昂，取样通过随机选取数据子集来降低计算负担。常见的取样方法有：
1)	随机取样 (Random Sampling)：均匀地从数据集中选取样本。
2)	替代或不替代取样 (Sampling with or without Replacement)：根据是否允许重复选择实例进行取样。
3)	分层取样 (Stratified Sampling)：将数据分成不同类别后，从每个类别中随机选取样本，以确保样本代表整体分布。[3]

### 6、Python代码实现：
import pandas as pd

#示例数据

data = {

    'width': [100, 200, 150],
    
    'height': [50, 100, 75],
    
    'spending': [300, 1500, 600],
    
    'customer_name': ['Alice', 'Bob', 'Charlie']
    
}

df = pd.DataFrame(data)

#1. 聚合

df['area'] = df['width'] * df['height']

#2. 离散化

bins = [0, 500, 1000, float('inf')]

labels = ['少', '正常', '多']

df['spending_category'] = pd.cut(df['spending'], bins=bins, labels=labels)

#3. 特征筛选

df_filtered = df[['area', 'spending_category']]  # 只保留有用特征

#4. 特征提取（已在聚合中完成）

#这里我们已经提取了'area'作为新特征

#5. 取样

sampled_df = df.sample(n=2, random_state=1)  # 随机取样2个样本


print(df_filtered)

print(sampled_df)


#### 输出：
    area spending_category

0   5000                 少

1  20000                 多

2  11250                正常

   width  height  spending customer_name   area spending_category

0    100      50       300         Alice   5000                 少

2    150      75       600       Charlie  11250                正常


## 对比
数据清洗与数据收集的目标不同。数据收集关注如何获取更多数据，而数据清洗专注于提高现有数据的质量。与数据集成相比，后者强调将来自不同来源的数据整合在一起，数据清洗则关注于消除数据中的噪声和错误。此外，数据清洗还与数据分析密切相关，高质量的数据是有效分析的基础。

## 深入扩展
常用的数据清洗工具包括Pandas（Python库）、OpenRefine（开源工具）、Excel、Knime（数据分析平台）、Trifacta（数据准备工具）、Tableau Prep（可视化工具）和Apache Spark（大规模处理框架），以及R语言（通过dplyr和tidyverse进行清洗）。
数据清洗的具体步骤通常包括：
### 1、去重
通过比较关键字段（如姓名、邮箱）识别并合并重复记录。
### 2、处理缺失值
对于缺失的数据，可以选择删除相应记录、用均值/中位数填补、或使用插值法等方法进行补全。
### 3、标准化
确保所有数据遵循相同的格式，例如将日期格式统一为“YYYY-MM-DD”，将国家名称标准化为ISO代码等。
### 4、异常检测
通过统计分析方法（如Z-score或IQR）检测并处理异常值，确保数据的合理性。
### 5、数据转换
对分类变量进行编码（如独热编码），对数值型数据进行归一化或标准化，以便适应不同的分析方法。

### 插值算法
数据清洗方式种类繁多，其中，笔者将以插值算法为例进行详细操作与解释。
在数据与模型的处理与分析中，在现有数据过少不足以支撑分析的进行时，便需要使用一些数学方法模拟产生一些新的而又比较靠谱的值来满足需求。于是便采用插值来进行数据的处理。
#### 插值法定义：
设函数![image](https://github.com/user-attachments/assets/a733eb10-8850-47dc-a2aa-80aab349627b)
在区间![image](https://github.com/user-attachments/assets/660ab1a0-ec60-4d06-b3e9-fe97292ad408)

上有定义 ，且已知在点 ![image](https://github.com/user-attachments/assets/9e409d13-ab96-4134-95bb-ab4b62b5bebf)

上的值分别为: ![image](https://github.com/user-attachments/assets/870b5fc9-278c-4843-9165-b12fc0b9b8c6)

若存在(简单 函数P(x),使 ![image](https://github.com/user-attachments/assets/a91b89d9-fa28-48a4-a8cb-a96866c37793)

则称 ![image](https://github.com/user-attachments/assets/12b91f6a-80dd-463f-a2a9-103ca6926ae5)
为![image](https://github.com/user-attachments/assets/ae8e7424-8376-457d-8c53-832ee4a74b05)
的插值函数，点 ![image](https://github.com/user-attachments/assets/c594c914-0a09-4ea5-a03d-264ae036ef66)
称为插值节点，包含插值节点的区间![image](https://github.com/user-attachments/assets/4c3c9329-d652-48c9-80f0-35b864b9f16f)
 称为插值区间，求插值函数![image](https://github.com/user-attachments/assets/d97b17be-c337-4842-9f14-f0d0abe40def)
的方法称为插值法。
插值法又包括分段插值、插值多项式、三角插值……其中又多用拉格朗日插值多项式![image](https://github.com/user-attachments/assets/551a8c01-f172-4f7d-a024-52ffa1e1d753)

与牛顿插值 ![image](https://github.com/user-attachments/assets/5bd8d29b-f4e3-4249-9559-c7483268e480)

但多项式插值法中存在着一个最大的问题就是龙格现象（Runge phenomenon）。高次插值会产生龙格现象，即在两端处波动极大，产生明显的震荡。于是多采用分段二次插值或分段三次插值以提高插值精度。与拉格朗日插值法相比，牛顿插值法的计算过程县有继承性。(牛顿插值法每次插值只和前n项的值有关，这样每次只要在原来的函数上添加新的项，就能够产生新的函数)但是牛顿插值也存在龙格现象的问题。
埃尔米特（Hermite）插值是具有节点的导数值约束的插值，直接使用Hermite插值得到的多项式次数较高，也存在着龙格现象。因此在实际应用中，往往使用分段三次 Hermite 插值多项式(PCHIP)。

#### 在Matlab中进行实操
%分段三次埃尔米特插值

x = -pi:pi; y = sin(x); 

new_x = -pi:0.1:pi;

p = pchip(x,y,new_x);

figure(1); 

plot(x, y, 'o', new_x, p, 'r-')

![image](https://github.com/user-attachments/assets/f8246074-bf9a-4a9b-86e4-61633cdeeb64)

 

% 三次样条插值和分段三次埃尔米特插值的对比
x = -pi:pi; 

y = sin(x); 

new_x = -pi:0.1:pi;

p1 = pchip(x,y,new_x);   %分段三次埃尔米特插值

p2 = spline(x,y,new_x);  %三次样条插值

figure(2);

plot(x,y,'o',new_x,p1,'r-',new_x,p2,'b-')

legend('样本点','三次埃尔米特插值','三次样条插值','Location','SouthEast') 

![image](https://github.com/user-attachments/assets/57f36727-f91d-4a9e-be77-3d1ac1e9755e)

 


% 人口预测
population=[133126,133770,134413,135069,135738,136427,137122,137866,138639, 139538];

year = 2009:2018;

p1 = pchip(year, population, 2019:2021)  %分段三次埃尔米特插值预测

p2 = spline(year, population, 2019:2021) %三次样条插值预测

figure(4);

plot(year, population,'o',2019:2021,p1,'r*-',2019:2021,p2,'bx-')

legend('样本点','三次埃尔米特插值预测','三次样条插值预测','Location','SouthEast')

![image](https://github.com/user-attachments/assets/bb7f96b9-7ea2-4070-b391-e74f6c245dbd)

 

## 简明总结
数据清洗是提升数据质量的关键过程，包括去重、处理缺失值、标准化和异常检测等多个步骤。通过有效的数据清洗，可以确保后续数据分析和机器学习模型的准确性和可靠性，为数据驱动的决策提供坚实的基础。

## 参考文献
[1]	郭志懋, 周傲英. 数据质量和数据清洗研究综述［J］. 软件学报, 2002, 13(11): 2076-2082.

[2]	廖书妍. 数据清洗研究综述[J]. 计算机科技, 2020, (2361): 1-9. DOI: 10.14004/j.cnki.ckt.2020.2361.

[3]	Zafarani R., Abbasi M. A., Liu H. Social Media Mining: An Introduction [M]. 刘挺, 秦兵, 赵姸姸 译. 北京: 多人民邮电出版社, 2019.


